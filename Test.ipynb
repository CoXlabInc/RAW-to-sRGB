{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b06c7de-4b05-4d22-a2c9-23b69468f6b8",
   "metadata": {},
   "source": [
    "```\n",
    "#!/bin/bash\n",
    "echo \"Start to test the model....\"\n",
    "\n",
    "name=\"srrawjoint\"\n",
    "dataroot=\"./SRRAW\"\n",
    "\n",
    "python3 test.py \\\n",
    "    --model srrawjoint  --name $name      --dataset_name srraw  --pre_ispnet_coord False  --gcm_coord True \\\n",
    "    --load_iter 1       --save_imgs True  --calc_metrics True   --gpu_id 0        --visual_full_imgs False \\\n",
    "    --dataroot $dataroot  --scale 4\n",
    "\n",
    "python3 metrics.py  --name $name --dataroot $dataroot\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cfbd245-995b-4ec1-b08d-5cc7e0f11670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from options.test_options import TestOptions\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import Visualizer\n",
    "from tqdm import tqdm\n",
    "from util.util import calc_psnr as calc_psnr\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import OrderedDict as odict\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f35e5a-8f82-414f-852a-e2012dc3d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Bunch(object):\n",
    "    def __init__(self, adict):\n",
    "        self.__dict__.update(adict)\n",
    "\n",
    "opt = {'dataroot': './SRRAW', \n",
    "       'dataset_name': 'srraw', \n",
    "       'max_dataset_size': float('inf'), \n",
    "       'scale': 4, \n",
    "       'mode': 'RGB', \n",
    "       'imlib': 'cv2', \n",
    "       'batch_size': 4, #original 16 \n",
    "       'patch_size': 224, \n",
    "       'shuffle': True, \n",
    "       'num_dataloader': 4, \n",
    "       'drop_last': True, \n",
    "       'gpu_ids': [0], \n",
    "       'checkpoints_dir': './ckpt', \n",
    "       'verbose': True, \n",
    "       'suffix': '', \n",
    "       'name': 'srrawjoint', \n",
    "       'model': 'srrawjoint', \n",
    "       'load_path': '', \n",
    "       'load_iter': 1, \n",
    "       'gcm_coord': True, \n",
    "       'pre_ispnet_coord': False, \n",
    "       'chop': False, \n",
    "       'init_type': 'default', \n",
    "       'init_gain': 0.02, \n",
    "       'optimizer': 'Adam', \n",
    "       'niter': 1000, \n",
    "       'niter_decay': 0, \n",
    "       'lr_policy': 'step', \n",
    "       'lr_decay_iters': 200, \n",
    "       'lr': 0.0001, \n",
    "       'load_optimizers': False, \n",
    "       'weight_decay': 0, \n",
    "       'beta1': 0.9, \n",
    "       'beta2': 0.999, \n",
    "       'momentum': 0, \n",
    "       'alpha': 0.99, \n",
    "       'print_freq': 100, \n",
    "       'test_every': 1000, \n",
    "       'save_epoch_freq': 1, \n",
    "       'calc_metrics': True, \n",
    "       'save_imgs': True, \n",
    "       'visual_full_imgs': False, \n",
    "       'isTrain': False, \n",
    "       'serial_batches': False, \n",
    "       'input_nc': 3, \n",
    "       'output_nc': 3\n",
    "      }\n",
    "\n",
    "opt = Bunch(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2695485b-acc3-4b5a-ac3a-7db8988252c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./SRRAW/test/\n",
      "Starting to load images via multiple imreaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [srraw(test)] created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model [SRRAWJOINTModel] was created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/models/base_model.py:135: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(load_path, map_location=self.device)\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the model from ./ckpt/srrawjoint/SRResNet_model_1.pth\n",
      "All parameters are initialized using [./ckpt/srrawjoint/SRResNet_model_1.pth]\n",
      "loading the model from ./ckpt/srrawjoint/GCMModel_model_1.pth\n",
      "All parameters are initialized using [./ckpt/srrawjoint/GCMModel_model_1.pth]\n",
      "---------- Networks initialized -------------\n",
      "DataParallel(\n",
      "  (module): SRResNet(\n",
      "    (input_stage): Sequential(\n",
      "      (0): Conv2d(4, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
      "      (1): PReLU(num_parameters=64)\n",
      "    )\n",
      "    (resblock_1): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_2): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_3): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_4): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_5): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_6): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_7): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_8): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_9): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_10): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_11): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_12): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_13): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_14): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_15): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_16): Residual_Block(\n",
      "      (conv_1): Conv2d(64, 64, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (BatchNorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (Prelu): PReLU(num_parameters=64)\n",
      "      (conv_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (BatchNorm_1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (resblock_output): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "    )\n",
      "    (deconv_stage1): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (2): PReLU(num_parameters=256)\n",
      "    )\n",
      "    (deconv_stage2): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (2): PReLU(num_parameters=256)\n",
      "    )\n",
      "    (deconv_stage3): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (2): PReLU(num_parameters=256)\n",
      "    )\n",
      "    (deconv_output_stage): Conv2d(256, 3, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
      "  )\n",
      ")\n",
      "[Network SRResNet] Total number of parameters : 4.862 M\n",
      "DataParallel(\n",
      "  (module): GCMModel(\n",
      "    (guide_net): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(8, 32, kernel_size=(7, 7), stride=(2, 2))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (2): AdaptiveAvgPool2d(output_size=1)\n",
      "      (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (align_head): Sequential(\n",
      "      (0): Conv2d(5, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (align_base): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "    (align_tail): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "[Network GCMModel] Total number of parameters : 0.046 M\n",
      "-----------------------------------------------\n",
      "================================================================================\n",
      "srraw dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [17:34<10:46, 34.02s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.64 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 5.98 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m     45\u001b[0m time_val_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 46\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m     48\u001b[0m time_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m time_val_start\n",
      "File \u001b[0;32m/workspace/models/base_model.py:79\u001b[0m, in \u001b[0;36mBaseModel.test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     78\u001b[0m \t\u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 79\u001b[0m \t\t\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/models/srrawjoint_model.py:71\u001b[0m, in \u001b[0;36mSRRAWJOINTModel.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGCMModel_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetGCMModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_raw_demosaic, down_dslr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcm_coord)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGCMModel_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_wb(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGCMModel_out)\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetSRResNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_raw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, self.isp_coord[index])\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_wb(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_out)\n\u001b[1;32m     74\u001b[0m flow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_flow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGCMModel_out, down_dslr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetPWCNET)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/data_parallel.py:191\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, module_kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/models/srrawjoint_model.py:243\u001b[0m, in \u001b[0;36mSRResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    241\u001b[0m \tout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv_stage1(resblock_output)\n\u001b[1;32m    242\u001b[0m \tout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv_stage2(out)\n\u001b[0;32m--> 243\u001b[0m \tout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeconv_stage3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv_output_stage(out)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/activation.py:1512\u001b[0m, in \u001b[0;36mPReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.64 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 5.98 GiB is allocated by PyTorch, and 1.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "    if not isinstance(opt.load_iter, list):\n",
    "        load_iters = [opt.load_iter]\n",
    "    else:\n",
    "        load_iters = deepcopy(opt.load_iter)\n",
    "\n",
    "    if not isinstance(opt.dataset_name, list):\n",
    "        dataset_names = [opt.dataset_name]\n",
    "    else:\n",
    "        dataset_names = deepcopy(opt.dataset_name)\n",
    "    datasets = odict()\n",
    "    for dataset_name in dataset_names:\n",
    "        if opt.visual_full_imgs:\n",
    "            dataset = create_dataset(dataset_name, 'visual', opt)\n",
    "        else:\n",
    "            dataset = create_dataset(dataset_name, 'test', opt)\n",
    "        datasets[dataset_name] = tqdm(dataset)\n",
    "\n",
    "    for load_iter in load_iters:\n",
    "        opt.load_iter = load_iter\n",
    "        model = create_model(opt)\n",
    "        model.setup(opt)\n",
    "        model.eval()\n",
    "        # log_dir = '%s/%s/logs/log_epoch_%d.txt' % (\n",
    "        #         opt.checkpoints_dir, opt.name, load_iter)\n",
    "        # os.makedirs(os.path.split(log_dir)[0], exist_ok=True)\n",
    "        # f = open(log_dir, 'a')\n",
    "\n",
    "        for dataset_name in dataset_names:\n",
    "            opt.dataset_name = dataset_name\n",
    "            tqdm_val = datasets[dataset_name]\n",
    "            dataset_test = tqdm_val.iterable\n",
    "            dataset_size_test = len(dataset_test)\n",
    "\n",
    "            print('='*80)\n",
    "            print(dataset_name + ' dataset')\n",
    "            tqdm_val.reset()\n",
    "\n",
    "            psnr = [0.0] * dataset_size_test\n",
    "\n",
    "            time_val = 0\n",
    "            for i, data in enumerate(tqdm_val):\n",
    "                torch.cuda.empty_cache()\n",
    "                model.set_input(data)\n",
    "                torch.cuda.synchronize()\n",
    "                time_val_start = time.time()\n",
    "                model.test()\n",
    "                torch.cuda.synchronize()\n",
    "                time_val += time.time() - time_val_start\n",
    "                res = model.get_current_visuals()\n",
    "\n",
    "                if opt.visual_full_imgs:\n",
    "                    folder_dir = './ckpt/%s/visual_fullres' % (opt.name)\n",
    "                    os.makedirs(folder_dir, exist_ok=True)\n",
    "                    save_dir = '%s/%s.png' % (folder_dir, os.path.basename(data['fname'][0]).split('.')[0])\n",
    "                    dataset_test.imio.write(np.array(res['data_out'][0].cpu()).astype(np.uint8), save_dir)\n",
    "\n",
    "                if opt.calc_metrics:\n",
    "                    psnr[i] = calc_psnr(res['dslr_warp'], res['data_out']*res['dslr_mask']/255.)\n",
    "                \n",
    "                if opt.save_imgs:\n",
    "                    folder_dir = './ckpt/%s/output' % (opt.name)  \n",
    "                    os.makedirs(folder_dir, exist_ok=True)\n",
    "                    save_dir = '%s/%s.png' % (folder_dir, os.path.basename(data['fname'][0]).split('.')[0])\n",
    "                    dataset_test.imio.write(np.array(res['data_out'][0].cpu()).astype(np.uint8), save_dir)\n",
    "\n",
    "                    folder_dir = './ckpt/%s/warp_gt' % (opt.name)  \n",
    "                    os.makedirs(folder_dir, exist_ok=True)\n",
    "                    save_dir = '%s/%s.png' % (folder_dir, os.path.basename(data['fname'][0]).split('.')[0])\n",
    "                    dataset_test.imio.write(np.array(res['dslr_warp'][0].cpu()).astype(np.uint8), save_dir)\n",
    "\n",
    "                    folder_dir = './ckpt/%s/warp_gt_mask' % (opt.name) \n",
    "                    os.makedirs(folder_dir, exist_ok=True)\n",
    "                    save_dir = '%s/%s.png' % (folder_dir, os.path.basename(data['fname'][0]).split('.')[0])\n",
    "                    dataset_test.imio.write(np.array(res['dslr_mask'][0].cpu()).astype(np.uint8), save_dir)\n",
    "\n",
    "            avg_psnr = '%.2f'%np.mean(psnr)\n",
    "\n",
    "            # f.write('dataset: %s, PSNR: %s, Time: %.3f sec.\\n'\n",
    "            #         % (dataset_name, avg_psnr, time_val))\n",
    "            print('Time: %.3f s AVG Time: %.3f ms PSNR: %s\\n' % (time_val, time_val/dataset_size_test*1000, avg_psnr))\n",
    "        #     f.flush()\n",
    "        #     f.write('\\n')\n",
    "        # f.close()\n",
    "    for dataset in datasets:\n",
    "        datasets[dataset].close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50dc2cc-5c9c-433b-9f65-2cf496934f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
